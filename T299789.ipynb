{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook stores a list of unillustrated articles with suggested images in hdfs. See https://phabricator.wikimedia.org/T299789\n",
    "\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import os.path\n",
    "from os import path\n",
    "from wmfdata.spark import get_session\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in the full snapshot date\n",
    "snapshot = '2022-01-24'\n",
    "reg = r'^([\\w]+-[\\w]+)'\n",
    "short_snapshot = re.match(reg, snapshot).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    }
   ],
   "source": [
    "# We use wmfdata boilerplate to init a spark session.\n",
    "# Under the hood the library uses findspark to initialise\n",
    "# Spark's environment. pyspark imports will be available \n",
    "# after initialisation\n",
    "spark = get_session(type='regular', app_name=\"T299789\")\n",
    "import pyspark\n",
    "import pyspark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data stored by previous scripts\n",
    "\n",
    "commons_file_pages = spark.read.parquet('hdfs:/user/mfossati/commons_file_pages')\n",
    "commons_file_pages.createOrReplaceTempView('commons_file_pages')\n",
    "commons_files_related_wikidata_items = spark.read.parquet('commons_files_related_wikidata_items')\n",
    "commons_files_related_wikidata_items.createOrReplaceTempView('commons_files_related_wikidata_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather suggestions based on p18, p373 and lead image (previously written to a parquet, which we parse here)\n",
    "#\n",
    "# Also gather suggestions based on depicts\n",
    "# \n",
    "# Also determine a confidence score based on the source of the match, see https://phabricator.wikimedia.org/T301687\n",
    "\n",
    "\n",
    "query=\"\"\"WITH suggestion_qid_p18 AS\n",
    "(\n",
    " SELECT cfp.page_title, EXPLODE(cw.reverse_p18) as wikidataId, 'istype-wikidata-image' as source, NULL as found_on \n",
    " FROM commons_files_related_wikidata_items cw\n",
    " JOIN commons_file_pages cfp\n",
    " ON cfp.page_id=cw.page_id\n",
    " WHERE cw.reverse_p18 IS NOT NULL\n",
    "),\n",
    "suggestion_qid_p373 AS\n",
    "(\n",
    " SELECT page_title, SPLIT(info,\"\\\\\\|\")[0] as wikidataId, 'istype-wikidata-commons-category' as source, NULL as found_on\n",
    " FROM \n",
    " (\n",
    "     SELECT cfp.page_title as page_title, EXPLODE(cw.reverse_p373) as info\n",
    "     FROM commons_files_related_wikidata_items cw\n",
    "     JOIN commons_file_pages cfp\n",
    "     ON cfp.page_id=cw.page_id\n",
    "     WHERE cw.reverse_p373 IS NOT NULL\n",
    " )\n",
    "),\n",
    "suggestion_qid_leadImage AS\n",
    "(\n",
    " SELECT page_title, \n",
    " SPLIT(info,\"\\\\\\|\")[0] as wikidataId,\n",
    " 'istype-lead-image' as source, \n",
    " collect_set(SPLIT(info,\"\\\\\\|\")[1]) as found_on\n",
    " FROM \n",
    " (\n",
    "     SELECT cfp.page_title as page_title, EXPLODE(cw.container_page_qids) as info\n",
    "     FROM commons_files_related_wikidata_items cw\n",
    "     JOIN commons_file_pages cfp\n",
    "     ON cfp.page_id=cw.page_id\n",
    "     WHERE cw.container_page_qids IS NOT NULL\n",
    " )\n",
    " GROUP BY page_title, wikidataId\n",
    "),\n",
    "commons_statements AS \n",
    "( \n",
    " SELECT id AS mId,\n",
    " EXPLODE(statements) AS statement \n",
    " FROM structured_data.commons_entity WHERE snapshot='\"\"\"+snapshot+\"\"\"' \n",
    "),\n",
    "suggestion_qid_commons AS\n",
    "(\n",
    " SELECT DISTINCT from_json(statement.mainsnak.datavalue.value, 'entityType STRING, numericId INT, id STRING').id AS wikidataId,\n",
    " cfp.page_title,\n",
    " 'istype-depicts' as source,\n",
    " NULL as found_on\n",
    " FROM commons_statements cs\n",
    " JOIN commons_file_pages cfp \n",
    " ON cfp.page_id=SUBSTRING( cs.mId, 2 )\n",
    " WHERE statement.mainsnak.property IN ('P180', 'P6243', 'P921')\n",
    ")\n",
    "SELECT wikidataId, page_title AS suggestion, source, found_on, 90 as confidence FROM suggestion_qid_p18\n",
    "UNION \n",
    "SELECT wikidataId, page_title AS suggestion, source, found_on, 80 as confidence FROM suggestion_qid_p373\n",
    "UNION\n",
    "SELECT wikidataId, page_title AS suggestion, source, found_on, 80 as confidence FROM suggestion_qid_leadImage\n",
    "UNION\n",
    "SELECT wikidataId, page_title AS suggestion, source, found_on, 70 as confidence FROM suggestion_qid_commons\n",
    "\"\"\"\n",
    "wdSuggestionsDF = spark.sql(query)\n",
    "wdSuggestionsDF.createOrReplaceTempView(\"all_suggestions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles corresponding to wikidata items that are instances of lists, or years, or names, etc should NOT be illustrated, so filter out suggestions associated with those items\n",
    "\n",
    "unillustratable = [\n",
    "    \"Q577\", # year\n",
    "    \"Q29964144\", # calendar year\n",
    "    \"Q14795564\", # recurrent timeframe\n",
    "    \"Q3311614\", # century leap year\n",
    "    \"Q101352\", # family name\n",
    "    \"Q82799\", # name\n",
    "    \"Q4167410\", # list\n",
    "    \"Q21199\", # natural number\n",
    "    \"Q28920044\", # positive integer\n",
    "    \"Q28920052\", # non negative integer\n",
    "]\n",
    "query = \"\"\"\n",
    "SELECT wikidataId, suggestion, \n",
    "collect_set(source) AS sources, \n",
    "found_on, \n",
    "collect_set(from_json(claim.mainSnak.dataValue.value, 'entityType STRING, numericId INT, id STRING').id) as instance_of,\n",
    "MAX(confidence) as confidence_score\n",
    "FROM all_suggestions as\n",
    "JOIN wmf.wikidata_entity we \n",
    "ON as.wikidataId=we.id\n",
    "LATERAL VIEW OUTER explode(we.claims) c AS claim\n",
    "WHERE we.typ='item'\n",
    "AND claim.mainSnak.property='P31'\n",
    "AND we.snapshot='\"\"\"+snapshot+\"\"\"'\n",
    "AND from_json(claim.mainSnak.dataValue.value, 'entityType STRING, numericId INT, id STRING').id NOT IN ('\"\"\" + \"','\".join(unillustratable) + \"\"\"')\n",
    "GROUP BY wikidataId,suggestion,found_on\n",
    "ORDER BY wikidataId,suggestion\n",
    "\"\"\"\n",
    "fsDF = spark.sql(query)\n",
    "fsDF.createOrReplaceTempView(\"filtered_suggestions\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages=['ruwiki', 'ptwiki']\n",
    "for wiki in languages:\n",
    "    # get all suggestions for unillustrated articles on the wiki\n",
    "    query = \"\"\"WITH illustrated_pages AS\n",
    "    (\n",
    "        SELECT DISTINCT il_from\n",
    "        FROM wmf_raw.mediawiki_imagelinks il\n",
    "        JOIN commons_file_pages cfp\n",
    "        ON cfp.page_title=il.il_to\n",
    "        WHERE il.wiki_db='\"\"\"+wiki+\"\"\"' \n",
    "        AND il.snapshot='\"\"\"+short_snapshot+\"\"\"' \n",
    "    )\n",
    "    SELECT p.wiki_db,p.page_id,p.page_title,fs.suggestion,fs.sources,fs.found_on,fs.instance_of,fs.confidence_score,current_timestamp() as timestamp\n",
    "    FROM wmf_raw.mediawiki_page p\n",
    "    LEFT ANTI JOIN illustrated_pages il\n",
    "    ON il.il_from=p.page_id\n",
    "    JOIN wmf.wikidata_item_page_link wipl\n",
    "    ON p.page_id=wipl.page_id\n",
    "    JOIN filtered_suggestions fs\n",
    "    ON wipl.item_id=fs.wikidataId\n",
    "    WHERE p.page_namespace=0 \n",
    "    AND page_is_redirect=0 \n",
    "    AND p.wiki_db='\"\"\"+wiki+\"\"\"' \n",
    "    AND p.snapshot='\"\"\"+short_snapshot+\"\"\"'\n",
    "    AND wipl.wiki_db='\"\"\"+wiki+\"\"\"'\n",
    "    AND wipl.snapshot='\"\"\"+snapshot+\"\"\"'\n",
    "    \"\"\"\n",
    "    suggestionsDF = spark.sql(query).cache()\n",
    "    # NOTE: suggestions.all.<wiki> contains all the suggestions data that is needed by cassandra for https://phabricator.wikimedia.org/T299885 for an individual wiki, so hopefully \n",
    "    # all we'll need to do for that ticket is write the data to hdfs\n",
    "    suggestionsDF.write.mode('overwrite').parquet('suggestions.all.' + wiki)\n",
    "    suggestionsDF.createOrReplaceTempView(\"suggestions_for_wiki\")\n",
    "    \n",
    "    # Get all files with a suggestion, and write to a file for import into the search index\n",
    "    # \n",
    "    # NOTE: this is a SUPER hacky way of testing if we already have data for this wiki. \n",
    "    # @todo replace with fsspec\n",
    "    proc = subprocess.Popen(['hadoop', 'fs', '-test', '-e', 'suggestions.weighted_tags.' + wiki ])\n",
    "    proc.communicate()\n",
    "    if proc.returncode != 0:\n",
    "        # If we DO have previous data for this wiki load it up and use it to find suggestions that\n",
    "        # are in the new list but not in the old, so the old ones can be deleted from the index\n",
    "        #\n",
    "        # NOTE: this does not work!!!\n",
    "        #\n",
    "        # Lazy-evaluation in spark makes things weird - once we've have told spark that we're doing an overwrite on a file it deletes it immediately\n",
    "        # Will have to think of another way of doing this (maybe date-based?)\n",
    "        pageIdsWithSuggestionsPrevious = spark.read.parquet('suggestions.weighted_tags.' + wiki)\n",
    "        pageIdsWithSuggestionsPrevious.createOrReplaceTempView('previous_weighted_tags')\n",
    "        query = \"\"\"WITH writes as \n",
    "        (\n",
    "         SELECT DISTINCT s.page_id,'\"\"\" + wiki + \"\"\"' as wiki,'recommendation.image/' as tag, collect_set('exists|1') as values\n",
    "         FROM suggestions_for_wiki as s\n",
    "         GROUP BY s.page_id\n",
    "        ),\n",
    "        previous as \n",
    "        (\n",
    "         SELECT DISTINCT p.page_id, EXPLODE(p.values) as previous_value\n",
    "         FROM previous_weighted_tags p\n",
    "         LEFT ANTI JOIN suggestions_for_wiki as s\n",
    "         ON p.page_id=s.page_id\n",
    "        )\n",
    "        SELECT page_id,'\"\"\" + wiki + \"\"\"' as wiki,'recommendation.image/' as tag, collect_set('__DELETE_GROUPING__') as values \n",
    "        FROM previous \n",
    "        WHERE previous_value!='__DELETE_GROUPING__' \n",
    "        GROUP BY page_id\n",
    "        UNION\n",
    "        SELECT * FROM writes\n",
    "        \"\"\"  \n",
    "        pageIdsWithSuggestions = spark.sql(query)\n",
    "        # NOTE: this should satisfy the requirements for https://phabricator.wikimedia.org/T299884\n",
    "        # (or would if the part above actually worked)\n",
    "        pageIdsWithSuggestions.write.mode('overwrite').parquet('suggestions.weighted_tags.' + wiki)\n",
    "    else:\n",
    "        query = \"\"\"\n",
    "        SELECT DISTINCT s.page_id,'\"\"\" + wiki + \"\"\"' as wiki,'recommendation.image/' as tag, collect_set('exists|1') as values\n",
    "        FROM suggestions_for_wiki as s\n",
    "        GROUP BY s.page_id\n",
    "        \"\"\"\n",
    "        pageIdsWithSuggestions = spark.sql(query)\n",
    "        pageIdsWithSuggestions.write.mode('overwrite').parquet('suggestions.weighted_tags.' + wiki)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
